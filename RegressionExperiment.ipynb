{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as ds  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Loss(x_train,y_train,w,lmd):\n",
    "    temp1=0\n",
    "    n=0\n",
    "    for x_sample, y_sample in zip(x_train,y_train):\n",
    "        s=-y_sample*np.dot(x_sample,w)\n",
    "        temp1+=np.log(1+np.exp(0))\n",
    "        n+=1\n",
    "    loss=temp1/n+lmd*np.dot(w.T,w)/2\n",
    "    return loss\n",
    "\n",
    "def Grad(x_train,y_train,w,lmd):\n",
    "    temp2=0\n",
    "    n=0\n",
    "    for x_sample, y_sample in zip(x_train,y_train):\n",
    "        temp2+=y_sample*x_sample/(1 + np.exp(np.dot(y_sample, np.dot(x_sample, w))))\n",
    "        n+=1\n",
    "    grad=-temp2/n+lmd*w\n",
    "    return grad\n",
    "\n",
    "def NAG(x_train,y_train,w,lmd,v,h):  \n",
    "    g=Grad(x_train,y_train,w,lmd)\n",
    "    v=lmd*v+h*g\n",
    "    w=w-v\n",
    "    return w\n",
    "    \n",
    "def RMSProp(x_train,y_train,w,lmd,e,h,G):\n",
    "    g=Grad(x_train,y_train,w,lmd)\n",
    "    G=lmd*G+np.dpt(1-lmd,np.dot(g,g))\n",
    "    w=w-np.dot(h/np.sqrt(G+e),g)\n",
    "    return w\n",
    "\n",
    "def AdaDelta(x_train,y_train,w,lmd,e,dt,G):\n",
    "    g=Grad(x_t,y_t,w,lmd)\n",
    "    G=lmd*G+np.dpt(1-lmd,np.dot(g,g))\n",
    "    dw=-np.dot(np.sqrt(dt+e)/np.sqrt(G+e),g)\n",
    "    w=w+dw\n",
    "    dt=lmd*dt+np.dot(1-lmd,np.dot(dw,dw))\n",
    "    return w\n",
    "\n",
    "def Adam(x_train,y_train,w,lmd,b,m,e,h,G):\n",
    "    g=Grad(x_t,y_t,w,lmd)\n",
    "    m=b*m+(1-b)*g\n",
    "    G=lmd*G+np.dpt(1-lmd,np.dot(g,g))\n",
    "    a1=h*(np.sqrt(1-lmd)/(1-b))\n",
    "    lmd*=lmd\n",
    "    b*=b\n",
    "    w=w-a1*(m/np.sqrt(G+e))\n",
    "    return w\n",
    "\n",
    "def iterationNAG(x_train,x_validation,y_train,y_validation,w,lmd,v,h):\n",
    "    itera = 100\n",
    "    lr = 0.0001\n",
    "    train_loss=[]\n",
    "    validation_loss=[]\n",
    "    for i in range(itera):\n",
    "        loss_t=Loss(x_train,y_train,w,lmd)\n",
    "        tx,ty=x_train.shape\n",
    "        lt=loss_t[0,0]/tx\n",
    "        train_loss.append(lt)\n",
    "        loss_v=Loss(x_validation,y_validation,w,lmd)\n",
    "        vx,vy=x_validation.shape\n",
    "        lv=loss_v[0,0]/vx\n",
    "        validation_loss.append(lv)\n",
    "        w=NAG(x_train,y_train,w,lmd,v,h)\n",
    "    return w,train_loss,validation_loss\n",
    "\n",
    "def iterationRMSProp(x_train,x_validation,y_train,y_validation,w,lmd,e,h,G):\n",
    "    itera = 100\n",
    "    lr = 0.0001\n",
    "    train_loss=[]\n",
    "    validation_loss=[]\n",
    "    for i in range(itera):\n",
    "        loss_t=Loss(x_train,y_train,w,lmd)\n",
    "        tx,ty=x_train.shape\n",
    "        lt=loss_t[0,0]/tx\n",
    "        train_loss.append(lt)\n",
    "        loss_v=Loss(x_validation,y_validation,w,lmd)\n",
    "        vx,vy=x_validation.shape\n",
    "        lv=loss_v[0,0]/vx\n",
    "        validation_loss.append(lv)\n",
    "        w=RMSProp(x_train,y_train,w,lmd,e,h,G)\n",
    "    return w,train_loss,validation_loss\n",
    "\n",
    "def iterationAdaDelta(x_train,x_validation,y_train,y_validation,w,lmd,e,dt,G):\n",
    "    itera = 100\n",
    "    lr = 0.0001\n",
    "    train_loss=[]\n",
    "    validation_loss=[]\n",
    "    for i in range(itera):\n",
    "        loss_t=Loss(x_train,y_train,w,lmd)\n",
    "        tx,ty=x_train.shape\n",
    "        lt=loss_t[0,0]/tx\n",
    "        train_loss.append(lt)\n",
    "        loss_v=Loss(x_validation,y_validation,w,lmd)\n",
    "        vx,vy=x_validation.shape\n",
    "        lv=loss_v[0,0]/vx\n",
    "        validation_loss.append(lv)\n",
    "        w=AdaDelta(x_train,y_train,w,lmd,e,dt,G)\n",
    "    return w,train_loss,validation_loss\n",
    "\n",
    "def iterationAdam(x_train,x_validation,y_train,y_validation,w,lmd,b,m,e,h,G):\n",
    "    itera = 100\n",
    "    lr = 0.0001\n",
    "    train_loss=[]\n",
    "    validation_loss=[]\n",
    "    for i in range(itera):\n",
    "        loss_t=Loss(x_train,y_train,w,lmd)\n",
    "        tx,ty=x_train.shape\n",
    "        lt=loss_t[0,0]/tx\n",
    "        train_loss.append(lt)\n",
    "        loss_v=Loss(x_validation,y_validation,w,lmd)\n",
    "        vx,vy=x_validation.shape\n",
    "        lv=loss_v[0,0]/vx\n",
    "        validation_loss.append(lv)\n",
    "        w=Adam(x_train,y_train,w,lmd,b,m,e,h,G)\n",
    "    return w,train_loss,validation_loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x_train, y_train = ds.load_svmlight_file('E:/test/train.txt')\n",
    "    x_validation, y_validation = ds.load_svmlight_file('E:/test/val.txt')\n",
    "    x_train = x_train.toarray()\n",
    "    x_t=[]\n",
    "    y_t=[]\n",
    "    yl=len(y_train)\n",
    "    y_train=y_train.reshape(yl,1)\n",
    "    n_sample,n_feature=x_train.shape\n",
    "    for sample in np.random.randint(1,100,[1,25]):\n",
    "        x_t.append(x_train[sample])\n",
    "        y_t.append(y_train[sample])\n",
    "    w0=np.zeros(shape=(n_feature,1))\n",
    "    lmd=0.9\n",
    "    v=0.01\n",
    "    h=0.01\n",
    "    wn,train_lossn,validation_lossn=iterationNAG(x_train,x_validation,y_train,y_validation,w0,lmd,v,h)\n",
    "    plt.plot(validation_lossn, label='validation loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    w1=np.zeros(shape=(n_feature,1))\n",
    "    h1=0.001\n",
    "    e=0\n",
    "    G=0.001\n",
    "    wr,train_lossr,validation_lossr=iterationRMSProp(x_train,x_validation,y_train,y_validation,w1,lmd,e,h1,G)\n",
    "    plt.plot(validation_lossr, label='validation loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    w2=np.zeros(shape=(n_feature,1))\n",
    "    lmd_1=0.95\n",
    "    dt=0\n",
    "    wa,train_lossa,validation_lossa=iterationAdaDelta(x_train,x_validation,y_train,y_validation,w2,lmd_1,e,dt,G) \n",
    "    plt.plot(validation_lossa, label='validation loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    w3=np.zeros(shape=(n_feature,1))\n",
    "    lmd_2=0.999\n",
    "    b=0.9\n",
    "    m=0\n",
    "    wam,train_lossam,validation_lossam=iterationAdam(x_train,x_validation,y_train,y_validation,w3,lmd_2,b,m,e,h1,G)\n",
    "    plt.plot(validation_lossam, label='validation loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
